{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atqZGIIyNSBb"
      },
      "source": [
        "#**Практическое задание №1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGBk36LpukIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bab33d10-92da-4d7b-a39d-7fccde15b288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.13.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.66.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.6.6\n",
            "    Uninstalling gdown-4.6.6:\n",
            "      Successfully uninstalled gdown-4.6.6\n",
            "Successfully installed gdown-4.7.1\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tqdm\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G5KkA1Nu5M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb4f1502-dd1b-4a58-b29e-96a134451a43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab2yCwDm7Fqb"
      },
      "outputs": [],
      "source": [
        "EVALUATE_ONLY = True\n",
        "TEST_ON_LARGE_DATASET = True\n",
        "TISSUE_CLASSES = ('ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM')\n",
        "DATASETS_LINKS = {\n",
        "    # Закоментированные строки из оригинального ноутбука\n",
        "    # К сожалению оригинальные ссылки не работают из-за большого кол-ва скачиваний\n",
        "\n",
        "\n",
        "    # 'train': '1XtQzVQ5XbrfxpLHJuL0XBGJ5U7CS-cLi',\n",
        "    'train': '1ccAgGUs43hA6hf9rpV8fi84VLv_2uW8a',\n",
        "    # 'train_small': '1qd45xXfDwdZjktLFwQb-et-mAaFeCzOR',\n",
        "    'train_small': '14bpdxgb55YzBuVGORq3imnLadTIuKTEo',\n",
        "    # 'train_tiny': '1I-2ZOuXLd4QwhZQQltp817Kn3J0Xgbui',\n",
        "    'train_tiny': '18jKz6GfnilfIYZHT-sASvPfU1BH6p2OU',\n",
        "    # 'test': '1RfPou3pFKpuHDJZ-D9XDFzgvwpUBFlDr',\n",
        "    'test': '1brH5TzbTNUPKz3yoWS_RD4FW1xJc-dEK',\n",
        "    # 'test_small': '1wbRsog0n7uGlHIPGLhyN-PMeT2kdQ2lI',\n",
        "    'test_small': '1FAULgTFgf-60lziVOGFABmveXcrOKFUH',\n",
        "    # 'test_tiny': '1viiB0s041CNsAK4itvX8PnYthJ-MDnQc'\n",
        "    'test_tiny': '1bOavoin0mTiBhx8AYZhIkAa3YhEinbLa'\n",
        "}\n",
        "IMG_HEIGHT = 224\n",
        "IMG_WIDTH = 224\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "SHUFFLE_BUFFER_SIZE = 100\n",
        "TRAIN_RATIO = 0.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLHQhqiSIyvK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import gdown\n",
        "import cv2\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N169efsw1ej"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "class Dataset:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.is_loaded = False\n",
        "        url = f\"https://drive.google.com/uc?export=download&confirm=pbef&id={DATASETS_LINKS[name]}\"\n",
        "        output = f'{name}.npz'\n",
        "        gdown.download(url, output, quiet=False)\n",
        "        print(f'Loading dataset {self.name} from npz.')\n",
        "        np_obj = np.load(f'{name}.npz')\n",
        "        self.images = np_obj['data']\n",
        "        self.labels = np_obj['labels']\n",
        "        self.n_files = self.images.shape[0]\n",
        "        self.is_loaded = True\n",
        "        print(f'Done. Dataset {name} consists of {self.n_files} images.')\n",
        "        self.train_inds = np.random.choice(self.n_files, int(self.n_files * TRAIN_RATIO), replace=False)\n",
        "        self.val_inds = np.setdiff1d(np.arange(self.n_files), self.train_inds)\n",
        "\n",
        "    def image(self, i):\n",
        "        # read i-th image in dataset and return it as numpy array\n",
        "        if self.is_loaded:\n",
        "            return self.images[i, :, :, :]\n",
        "\n",
        "    def images_seq(self, n=None):\n",
        "        # sequential access to images inside dataset (is needed for testing)\n",
        "        for i in range(self.n_files if not n else n):\n",
        "            yield self.image(i)\n",
        "\n",
        "    def random_image_with_label(self):\n",
        "        # get random image with label from dataset\n",
        "        i = np.random.randint(self.n_files)\n",
        "        return self.image(i), self.labels[i]\n",
        "\n",
        "    def random_batch_with_labels(self, n):\n",
        "        # create random batch of images with labels (is needed for training)\n",
        "        indices = np.random.choice(self.n_files, n)\n",
        "        imgs = []\n",
        "        for i in indices:\n",
        "            img = self.image(i)\n",
        "            imgs.append(self.image(i))\n",
        "        logits = np.array([self.labels[i] for i in indices])\n",
        "        return np.stack(imgs), logits\n",
        "\n",
        "    def random_batch_from_train_val(self, n, set_name='train'):\n",
        "        if set_name == 'train':\n",
        "            indices = np.random.choice(self.train_inds, n)\n",
        "        else:\n",
        "            indices = np.random.choice(self.val_inds, n)\n",
        "        imgs = []\n",
        "        for i in indices:\n",
        "            img = self.image(i)\n",
        "            imgs.append(self.image(i))\n",
        "        logits = np.array([self.labels[i] for i in indices])\n",
        "        return np.stack(imgs), logits\n",
        "\n",
        "\n",
        "    def image_with_label(self, i: int):\n",
        "        # return i-th image with label from dataset\n",
        "        return self.image(i), self.labels[i]\n",
        "\n",
        "    def train_val_index_split(self, train_ratio=0.8):\n",
        "        self.train_inds = np.random.choice(self.n_files, int(self.n_files * train_ratio), replace=False)\n",
        "        self.val_inds = np.setdiff1d(np.arange(self.n_files), self.train_inds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MySequence(tf.keras.utils.Sequence):\n",
        "    def __init__(self, dataset: Dataset, set_name='train') -> None:\n",
        "      super().__init__()\n",
        "      self.dataset = dataset\n",
        "      self.leny = dataset.n_files // BATCH_SIZE\n",
        "      self.set_name = set_name\n",
        "\n",
        "    def __len__(self):\n",
        "      return self.leny\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.dataset.random_batch_from_train_val(BATCH_SIZE, self.set_name)"
      ],
      "metadata": {
        "id": "gxR2BA_-wZak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5unQ7azTinCZ"
      },
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(gt: List[int], pred: List[int]):\n",
        "        assert len(gt) == len(pred), 'gt and prediction should be of equal length'\n",
        "        return sum(int(i[0] == i[1]) for i in zip(gt, pred)) / len(gt)\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy_balanced(gt: List[int], pred: List[int]):\n",
        "        return balanced_accuracy_score(gt, pred)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_all(gt: List[int], pred: List[int], info: str):\n",
        "        print(f'metrics for {info}:')\n",
        "        print('\\t accuracy {:.4f}:'.format(Metrics.accuracy(gt, pred)))\n",
        "        print('\\t balanced accuracy {:.4f}:'.format(Metrics.accuracy_balanced(gt, pred)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pkMiB6mJ7JQ"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        num_classes = len(TISSUE_CLASSES)\n",
        "\n",
        "        data_augmentation = keras.Sequential(\n",
        "            [\n",
        "                layers.RandomFlip(\"horizontal\",\n",
        "                                input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)),\n",
        "                layers.RandomRotation(0.1),\n",
        "                layers.RandomZoom(0.1),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        efficient_netb0 = keras.applications.EfficientNetB0(include_top=True, weights=None, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "\n",
        "        self.model = Sequential()\n",
        "        self.model.add(data_augmentation)\n",
        "        self.model.add(efficient_netb0)\n",
        "        self.model.add(layers.Dropout(0.2))\n",
        "        self.model.add(layers.Dense(128, activation='relu', kernel_regularizer='l2'))\n",
        "        self.model.add(layers.Dense(num_classes))\n",
        "\n",
        "        self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "    def save(self, name: str):\n",
        "        self.model.save(name)\n",
        "        shutil.make_archive(name, 'zip', name)\n",
        "        shutil.copy(f\"{name}.zip\", \"/content/drive/MyDrive/\")\n",
        "\n",
        "    def load(self, name: str):\n",
        "        name_to_id_dict = {\n",
        "            'best': '1ZVknWsqBF7giQb0b-vJauG6hD_RjZcbP'\n",
        "        }\n",
        "        output = f'{name}.zip'\n",
        "        gdown.download(f'https://drive.google.com/uc?id={name_to_id_dict[name]}', output, quiet=False)\n",
        "        zip_ref = zipfile.ZipFile(output, 'r')\n",
        "        zip_ref.extractall()\n",
        "        self.model = keras.models.load_model(f\"{name}\")\n",
        "        self.model.summary()\n",
        "\n",
        "    def train(self, dataset: Dataset):\n",
        "        train_seq = MySequence(dataset, set_name = 'train')\n",
        "        val_seq = MySequence(dataset, set_name = 'val')\n",
        "\n",
        "        checkpoint = ModelCheckpoint(\"new_model\", monitor='loss', verbose=1,\n",
        "                    save_best_only=True, mode='auto', period=1)\n",
        "\n",
        "        epochs=80\n",
        "        self.history = self.model.fit(\n",
        "            train_seq,\n",
        "            validation_data=val_seq,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            # steps_per_epoch = 15,\n",
        "            # validation_steps = 7,\n",
        "            callbacks=[checkpoint]\n",
        "        )\n",
        "\n",
        "    def continue_train(self, name: str, dataset: Dataset):\n",
        "        self.load(name)\n",
        "        self.train(dataset)\n",
        "\n",
        "    def test_on_dataset(self, dataset: Dataset, limit=None):\n",
        "        if limit is not None:\n",
        "            return self.model.predict(dataset.images[:int(dataset.n_files*limit)]).argmax(axis=-1)\n",
        "        return self.model.predict(dataset.images).argmax(axis=-1)\n",
        "\n",
        "\n",
        "    def test_on_image(self, img: np.ndarray):\n",
        "        prediction = self.model.predict(img)\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cTOuZD01Up6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eefdc400-5b33-4e1b-eeb0-114f8aae4173"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=pbef&id=1ccAgGUs43hA6hf9rpV8fi84VLv_2uW8a\n",
            "To: /content/train.npz\n",
            "100%|██████████| 2.10G/2.10G [00:21<00:00, 96.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset train from npz.\n",
            "Done. Dataset train consists of 18000 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=pbef&id=1brH5TzbTNUPKz3yoWS_RD4FW1xJc-dEK\n",
            "To: /content/test.npz\n",
            "100%|██████████| 525M/525M [00:02<00:00, 230MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset test from npz.\n",
            "Done. Dataset test consists of 4500 images.\n"
          ]
        }
      ],
      "source": [
        "d_train = Dataset('train')\n",
        "d_test = Dataset('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBi0XpXg8_wq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a968ef7d-f2ab-4398-9272-44146687e73c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional  (None, 1000)              5330571   \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1000)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               128128    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5459860 (20.83 MB)\n",
            "Trainable params: 5417837 (20.67 MB)\n",
            "Non-trainable params: 42023 (164.16 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1ZVknWsqBF7giQb0b-vJauG6hD_RjZcbP\n",
            "From (redirected): https://drive.google.com/uc?id=1ZVknWsqBF7giQb0b-vJauG6hD_RjZcbP&confirm=t&uuid=32ad2e47-6680-46e8-9f00-dc9fdc4caa09\n",
            "To: /content/best.zip\n",
            "100%|██████████| 60.6M/60.6M [00:01<00:00, 38.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional  (None, 1000)              5330571   \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1000)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               128128    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5459860 (20.83 MB)\n",
            "Trainable params: 5417837 (20.67 MB)\n",
            "Non-trainable params: 42023 (164.16 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Model()\n",
        "if not EVALUATE_ONLY:\n",
        "    model.train(d_train)\n",
        "    model.save('new_model')\n",
        "else:\n",
        "    model.load('best')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0AqmeLEKqrs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9ae683-09a2-40f7-b49c-03d9bf34e2b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 38s 2s/step\n",
            "metrics for 10% of test:\n",
            "\t accuracy 0.9800:\n",
            "\t balanced accuracy 0.9800:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
          ]
        }
      ],
      "source": [
        "# evaluating model on 10% of test dataset\n",
        "\n",
        "pred_1 = model.test_on_dataset(d_test, limit=0.1)\n",
        "Metrics.print_all(d_test.labels[:len(pred_1)], pred_1, '10% of test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjI_sbMi3TMY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57758a05-1985-43b0-c999-d84cc730fee9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "141/141 [==============================] - 352s 2s/step\n",
            "metrics for test:\n",
            "\t accuracy 0.9600:\n",
            "\t balanced accuracy 0.9600:\n"
          ]
        }
      ],
      "source": [
        "# evaluating model on full test dataset (may take time)\n",
        "if TEST_ON_LARGE_DATASET:\n",
        "    pred_2 = model.test_on_dataset(d_test)\n",
        "    Metrics.print_all(d_test.labels, pred_2, 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdY3uTt87tqv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d06c05e-777d-49cb-e920-466feca86244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential_2 (Sequential)   (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional  (None, 1000)              5330571   \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               128128    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5459860 (20.83 MB)\n",
            "Trainable params: 5417837 (20.67 MB)\n",
            "Non-trainable params: 42023 (164.16 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (uriginal): https://drive.google.com/uc?id=1ZVknWsqBF7giQb0b-vJauG6hD_RjZcbP\n",
            "From (redirected): https://drive.google.com/uc?id=1ZVknWsqBF7giQb0b-vJauG6hD_RjZcbP&confirm=t&uuid=70997d9d-33b2-4abc-a2a3-0c73b6b3e805\n",
            "To: /content/best.zip\n",
            "100%|██████████| 60.6M/60.6M [00:00<00:00, 147MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
            "                                                                 \n",
            " efficientnetb0 (Functional  (None, 1000)              5330571   \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1000)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               128128    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 1161      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5459860 (20.83 MB)\n",
            "Trainable params: 5417837 (20.67 MB)\n",
            "Non-trainable params: 42023 (164.16 KB)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&confirm=pbef&id=1bOavoin0mTiBhx8AYZhIkAa3YhEinbLa\n",
            "To: /content/test_tiny.npz\n",
            "100%|██████████| 10.6M/10.6M [00:00<00:00, 131MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset test_tiny from npz.\n",
            "Done. Dataset test_tiny consists of 90 images.\n",
            "3/3 [==============================] - 8s 2s/step\n",
            "metrics for test-tiny:\n",
            "\t accuracy 0.9556:\n",
            "\t balanced accuracy 0.9556:\n"
          ]
        }
      ],
      "source": [
        "final_model = Model()\n",
        "final_model.load('best')\n",
        "d_test_tiny = Dataset('test_tiny')\n",
        "pred = model.test_on_dataset(d_test_tiny)\n",
        "Metrics.print_all(d_test_tiny.labels, pred, 'test-tiny')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfX35zNSvFWn"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jxdA0v8Psyz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "7af69d84c46e0da4f71f361435e72c01e713b5d1fcbc89c051c042527a934273"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}